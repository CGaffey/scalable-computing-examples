{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For parallel processing\n",
    "import parsl\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "# helpers\n",
    "from grouputils import initialize_rasterizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "![](https://raw.githubusercontent.com/PermafrostDiscoveryGateway/viz-raster/develop/docs/images/raster_tldr.png)\n",
    "\n",
    "Today, we will create rasters from the regularly gridded staged data from yesterday (vector data, left side of diagram above). Each pixel of the raster will contain a stasticic calculated based on the underlying vector data for that pixel.\n",
    "\n",
    "The two statistics we calculate are:\n",
    "\n",
    "- number of IWP per pixel\n",
    "- proportion of pixel covered by IWP\n",
    "\n",
    "The vector data on the left are geopackages, which can be read in as simple geodataframes. The rasters take the geometry columns of all the geodataframes, which are all simple polygons, and represent them as pixels. \n",
    "\n",
    "Similar to the `TileStager` from the first step of the group project, in this step we will use a `RasterTiler` class, which we initialize using `initialize_rasterizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwp_rasterizer = initialize_rasterizer(\"/home/shares/example-pdg-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a look at the data we need to rasterize first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_paths = iwp_rasterizer.tiles.get_filenames_from_dir('staged')\n",
    "print(len(staged_paths), \"files to rasterize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `rasterize_vector` method on our `RasterTiler` class object, and pass it the first file in our list of staged files to rasterize just one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwp_rasterizer.rasterize_vector(staged_paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This went pretty quickly, but with thousands of files, it would still be beneficial to parallelize. Estimate the length of time it would take to process the files in series below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate duration of rasterization process\n",
    "print(round(4 * len(staged_paths)/60), \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterize in Parallel\n",
    "\n",
    "Given what you know about the limits of parallelization, discuss in your group whether you think this problem is:\n",
    "\n",
    "- cpu-bound\n",
    "- memory-bound\n",
    "- I/O-bound\n",
    "- network-bound\n",
    "\n",
    "How does it compare to the parallelization task from yesterday?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parallelizing this step over all 1,082 files, we will instead process the data in batches of files. Below is a function we can use to make batches of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because rasterization is relatively quick, we want each parsl \"task\" to process a batch of tiles.\n",
    "def make_batch(items, batch_size):\n",
    "    # Create batches of a given size from a list of items.\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `make_batch` function to make batches of 10 items each of our staged data (`staged_paths`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batches of 10 files\n",
    "batch_size = 10\n",
    "batches = make_batch(staged_paths, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set up your Parsl executor again, with `max_workers` set at 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Parsl executor\n",
    "activate_env = 'workon scomp2023-03-20'\n",
    "htex_local = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            max_workers=15,\n",
    "            provider=LocalProvider(\n",
    "                worker_init=activate_env\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "parsl.clear()\n",
    "parsl.load(htex_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up your Parsl app to run the `rasterize_vector` method in parallel. Remember that Parsl apps cannot rely on global variables or package imports, so you'll need to make sure to pass the app all of the variables it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Parsl app that uses the rasterize_vectors method\n",
    "@python_app\n",
    "def rasterize(staged_paths, rasterizer):\n",
    "    # Rasterize a batch of vector files\n",
    "    return rasterizer.rasterize_vectors(staged_paths, make_parents=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the app in parallel over all of the batches of files you created previously. Don't forget to add lines to shut down the executor and clear `parsl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the batches in parallel\n",
    "app_futures = []\n",
    "for batch in batches:\n",
    "    app_future = rasterize(batch, iwp_rasterizer)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to print message until all tiles have been rasterized\n",
    "[app_future.result() for app_future in app_futures]\n",
    "\n",
    "htex_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that you have the same number of GeoTIFF files as we do staged vector tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_paths = iwp_rasterizer.tiles.get_filenames_from_dir('geotiff')\n",
    "len(geotiff_paths) == len(staged_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 1:1 ratio because we only processed the highest resolution files: those at zoom-level 15."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Thinking back to the group project yesterday, how would you test whether this problem is CPU or I/O bound? One option would be to test whether adding more CPU decreases your computation time. Set up a test on a subset of the staged files (the first 500 is fine) and time the process at different `max_workers`. To make sure you don't overload the server, do not exceed 32 workers in your tests.\n",
    "\n",
    "Here are the results for three sets of tests where the `max_workers` was increased all the way to 88. We also varied the number of files in each batch, which effectively alters the degree of parallelism in our code. The result is a figure which looks remarkably like Amdahl's Law!\n",
    "\n",
    "![](images/parallel-performance.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scomp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0030ed2ed4c0609037967981d5426019e14a913516344490dbc8ffbbe92f86b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
